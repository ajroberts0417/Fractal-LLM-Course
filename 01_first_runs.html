<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Fractal-LLM-Course - Run a Large Language Model using the HuggingFace Transformers API.</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Fractal-LLM-Course - Run a Large Language Model using the HuggingFace Transformers API.">
<meta property="og:description" content="">
<meta property="og:site-name" content="Fractal-LLM-Course">
<meta name="twitter:title" content="Fractal-LLM-Course - Run a Large Language Model using the HuggingFace Transformers API.">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Fractal-LLM-Course</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Run a Large Language Model using the <a href="https://huggingface.co/docs/transformers/index">HuggingFace <code>Transformers</code></a> API.</li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#first-a-pipeline" id="toc-first-a-pipeline" class="nav-link" data-scroll-target="#first-a-pipeline">First, a Pipeline</a></li>
  </ul></li>
  <li><a href="#inspecting-the-classifier-notebook-style." id="toc-inspecting-the-classifier-notebook-style." class="nav-link" data-scroll-target="#inspecting-the-classifier-notebook-style.">Inspecting the <code>classifier</code>, notebook style.</a></li>
  <li><a href="#peeking-inside-the-pipeline" id="toc-peeking-inside-the-pipeline" class="nav-link" data-scroll-target="#peeking-inside-the-pipeline">Peeking inside the <code>pipeline</code></a>
  <ul class="collapse">
  <li><a href="#config-class" id="toc-config-class" class="nav-link" data-scroll-target="#config-class">Config class</a></li>
  <li><a href="#preprocessor-class" id="toc-preprocessor-class" class="nav-link" data-scroll-target="#preprocessor-class">Preprocessor class</a></li>
  <li><a href="#model-class" id="toc-model-class" class="nav-link" data-scroll-target="#model-class">Model class</a></li>
  </ul></li>
  <li><a href="#more-hf-magic" id="toc-more-hf-magic" class="nav-link" data-scroll-target="#more-hf-magic">More HF magic</a>
  <ul class="collapse">
  <li><a href="#more-hugging-face-magic" id="toc-more-hugging-face-magic" class="nav-link" data-scroll-target="#more-hugging-face-magic">More Hugging Face Magic</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/enzokro/Fractal-LLM-Course/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Run a Large Language Model using the <a href="https://huggingface.co/docs/transformers/index">HuggingFace <code>Transformers</code></a> API.</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The cells below are good defaults for development.</p>
<p>The <code>autoreload</code> lines help load libraries on the fly, while they are changing. This works well with the editable install we created via <code>pip install -e .</code><br>
This means we can edit the source code directly and have the change reflected live in the notebook.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
</div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Imagine we have a list of product review from our users. Now we want to find out whether those reviews were good or bad. It will take a lot of effort to manually go through and check each one. But, using an LLM, we can automatically get a label for a given product review.</p>
<p>How would this be useful? We could use it to find the more negative reviews to see where our product needs improving. Or, we can look at the more positive ones to see what weâ€™re doing right.</p>
<p>The broader task in NLP of figuring out a statementâ€™s tone is called <code>Sentiment Analysis</code>.</p>
<section id="first-a-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="first-a-pipeline">First, a Pipeline</h2>
<p>A HuggingFace model is based on 3 key pieces: 1. Config file.<br>
2. Preprocessor file.<br>
3. Model file.</p>
<p>The HuggingFace API gives us a way of automatically using these pieces directly: the <code>pipeline</code>.</p>
<p>Letâ€™s get right it and create a Sentiment Analysis <code>pipeline</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load in the pipeline object from huggingface</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create the sentiment analysis pipeline</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/cck/mambaforge/envs/llm_base/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 629/629 [00:00&lt;00:00, 1.06MB/s]
Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:34&lt;00:00, 7.76MB/s] 
Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00&lt;00:00, 432kB/s]
Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00&lt;00:00, 9.00MB/s]</code></pre>
</div>
</div>
<p>We can see in the output message above that HuggingFace automatically picked a decent, default model for us since we didnâ€™t specify one. Specifically, it chose a <a href="distilbert-base-uncased-finetuned-sst-2-english">distilbert model</a>.</p>
<p>We will learn more about what exactly <code>distilbert</code> is and how it works later on. For now, think of it as a useful NLP genie who can tell us how it feels about a given sentence.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># example from the HuggingFace tutorial</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>classifier(<span class="st">"We are very happy to show you the ðŸ¤— Transformers library."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[{'label': 'POSITIVE', 'score': 0.9997795224189758}]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># passing in several sentences at once, inside a python list</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> classifier([</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"We are very happy to show you the ðŸ¤— Transformers library."</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"We hope you don't hate it."</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I love Fractal! I'm so glad it's not a cult!"</span>, </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># print the output of each results</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> result <span class="kw">in</span> results:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"label: </span><span class="sc">{</span>result[<span class="st">'label'</span>]<span class="sc">}</span><span class="ss">, with score: </span><span class="sc">{</span><span class="bu">round</span>(result[<span class="st">'score'</span>], <span class="dv">4</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
label: POSITIVE, with score: 0.999</code></pre>
</div>
</div>
</section>
</section>
<section id="inspecting-the-classifier-notebook-style." class="level1">
<h1>Inspecting the <code>classifier</code>, notebook style.</h1>
<p>What is the <code>classifier</code>, exactly?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>classifier</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;transformers.pipelines.text_classification.TextClassificationPipeline&gt;</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">## showing the lookup's auto-complete</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># classifier.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">## viewing all of a class' methods and properties</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">dir</span>(classifier)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>['__abstractmethods__',
 '__call__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getstate__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__sizeof__',
 '__slots__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_abc_impl',
 '_batch_size',
 '_ensure_tensor_on_device',
 '_forward',
 '_forward_params',
 '_num_workers',
 '_postprocess_params',
 '_preprocess_params',
 '_sanitize_parameters',
 'binary_output',
 'call_count',
 'check_model_type',
 'default_input_names',
 'device',
 'device_placement',
 'ensure_tensor_on_device',
 'feature_extractor',
 'forward',
 'framework',
 'function_to_apply',
 'get_inference_context',
 'get_iterator',
 'image_processor',
 'iterate',
 'model',
 'modelcard',
 'postprocess',
 'predict',
 'preprocess',
 'return_all_scores',
 'run_multi',
 'run_single',
 'save_pretrained',
 'task',
 'tokenizer',
 'torch_dtype',
 'transform']</code></pre>
</div>
</div>
<p>Jupyter notebooks have powerful ways of inspecting and analyzing the code, as weâ€™re running it.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">## refresher</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>classifier</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;transformers.pipelines.text_classification.TextClassificationPipeline&gt;</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">## the power of asking questions</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># classifier? # help(classifier)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Signature:      classifier(*args, **kwargs)
Type:           TextClassificationPipeline
String form:    &lt;transformers.pipelines.text_classification.TextClassificationPipeline object&gt;
File:           ~/mambaforge/envs/llm_base/lib/python3.11/site-packages/transformers/pipelines/text_classification.py
Docstring:     
Text classification pipeline using any `ModelForSequenceClassification`. See the [sequence classification
examples](../task_summary#sequence-classification) for more information.

Example:

```python
&gt;&gt;&gt; from transformers import pipeline

&gt;&gt;&gt; classifier = pipeline(model="distilbert-base-uncased-finetuned-sst-2-english")
&gt;&gt;&gt; classifier("This movie is disgustingly good !")
[{'label': 'POSITIVE', 'score': 1.0}]

&gt;&gt;&gt; classifier("Director tried too much.")
[{'label': 'NEGATIVE', 'score': 0.996}]
```

Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)

This text classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:
`"sentiment-analysis"` (for classifying sequences according to positive or negative sentiments).

If multiple classification labels are available (`model.config.num_labels &gt;= 2`), the pipeline will run a softmax
over the results. If there is a single label, the pipeline will run a sigmoid over the result.

The models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See
the up-to-date list of available models on
[huggingface.co/models](https://huggingface.co/models?filter=text-classification).

Arguments:
    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):
        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from
        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.
    tokenizer ([`PreTrainedTokenizer`]):
        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from
        [`PreTrainedTokenizer`].
    modelcard (`str` or [`ModelCard`], *optional*):
        Model card attributed to the model for this pipeline.
    framework (`str`, *optional*):
        The framework to use, either `"pt"` for PyTorch or `"tf"` for TensorFlow. The specified framework must be
        installed.

        If no framework is specified, will default to the one currently installed. If no framework is specified and
        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is
        provided.
    task (`str`, defaults to `""`):
        A task-identifier for the pipeline.
    num_workers (`int`, *optional*, defaults to 8):
        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of
        workers to be used.
    batch_size (`int`, *optional*, defaults to 1):
        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
        the batch to use, for inference this is not always beneficial, please read [Batching with
        pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .
    args_parser ([`~pipelines.ArgumentHandler`], *optional*):
        Reference to the object in charge of parsing supplied pipeline parameters.
    device (`int`, *optional*, defaults to -1):
        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on
        the associated CUDA device id. You can pass native `torch.device` or a `str` too.
    binary_output (`bool`, *optional*, defaults to `False`):
        Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.

    return_all_scores (`bool`, *optional*, defaults to `False`):
        Whether to return all prediction scores or just the one of the predicted class.
    function_to_apply (`str`, *optional*, defaults to `"default"`):
        The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:

        - `"default"`: if the model has a single label, will apply the sigmoid function on the output. If the model
          has several labels, will apply the softmax function on the output.
        - `"sigmoid"`: Applies the sigmoid function on the output.
        - `"softmax"`: Applies the softmax function on the output.
        - `"none"`: Does not apply any function on the output.
Call docstring:
Classify the text(s) given as inputs.

Args:
    args (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`):
        One or several texts to classify. In order to use text pairs for your classification, you can send a
        dictionary containing `{"text", "text_pair"}` keys, or a list of those.
    top_k (`int`, *optional*, defaults to `1`):
        How many results to return.
    function_to_apply (`str`, *optional*, defaults to `"default"`):
        The function to apply to the model outputs in order to retrieve the scores. Accepts four different
        values:

        If this argument is not specified, then it will apply the following functions according to the number
        of labels:

        - If the model has a single label, will apply the sigmoid function on the output.
        - If the model has several labels, will apply the softmax function on the output.

        Possible values are:

        - `"sigmoid"`: Applies the sigmoid function on the output.
        - `"softmax"`: Applies the softmax function on the output.
        - `"none"`: Does not apply any function on the output.

Return:
    A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:

    - **label** (`str`) -- The label predicted.
    - **score** (`float`) -- The corresponding probability.

    If `top_k` is used, one such dictionary is returned per label.</code></pre>
</div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">## again, with feeling</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co"># classifier?? # ?? shows you the source code of the object</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Signature:      classifier(*args, **kwargs)
Type:           TextClassificationPipeline
String form:    &lt;transformers.pipelines.text_classification.TextClassificationPipeline object&gt;
File:           ~/mambaforge/envs/llm_base/lib/python3.11/site-packages/transformers/pipelines/text_classification.py
Source:        
@add_end_docstrings(
    PIPELINE_INIT_ARGS,
    r"""
        return_all_scores (`bool`, *optional*, defaults to `False`):
            Whether to return all prediction scores or just the one of the predicted class.
        function_to_apply (`str`, *optional*, defaults to `"default"`):
            The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:

            - `"default"`: if the model has a single label, will apply the sigmoid function on the output. If the model
              has several labels, will apply the softmax function on the output.
            - `"sigmoid"`: Applies the sigmoid function on the output.
            - `"softmax"`: Applies the softmax function on the output.
            - `"none"`: Does not apply any function on the output.
    """,
)
class TextClassificationPipeline(Pipeline):
    """
    Text classification pipeline using any `ModelForSequenceClassification`. See the [sequence classification
    examples](../task_summary#sequence-classification) for more information.

    Example:

    ```python
    &gt;&gt;&gt; from transformers import pipeline

    &gt;&gt;&gt; classifier = pipeline(model="distilbert-base-uncased-finetuned-sst-2-english")
    &gt;&gt;&gt; classifier("This movie is disgustingly good !")
    [{'label': 'POSITIVE', 'score': 1.0}]

    &gt;&gt;&gt; classifier("Director tried too much.")
    [{'label': 'NEGATIVE', 'score': 0.996}]
    ```

    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)

    This text classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:
    `"sentiment-analysis"` (for classifying sequences according to positive or negative sentiments).

    If multiple classification labels are available (`model.config.num_labels &gt;= 2`), the pipeline will run a softmax
    over the results. If there is a single label, the pipeline will run a sigmoid over the result.

    The models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See
    the up-to-date list of available models on
    [huggingface.co/models](https://huggingface.co/models?filter=text-classification).
    """

    return_all_scores = False
    function_to_apply = ClassificationFunction.NONE

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        self.check_model_type(
            TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES
            if self.framework == "tf"
            else MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES
        )

    def _sanitize_parameters(self, return_all_scores=None, function_to_apply=None, top_k="", **tokenizer_kwargs):
        # Using "" as default argument because we're going to use `top_k=None` in user code to declare
        # "No top_k"
        preprocess_params = tokenizer_kwargs

        postprocess_params = {}
        if hasattr(self.model.config, "return_all_scores") and return_all_scores is None:
            return_all_scores = self.model.config.return_all_scores

        if isinstance(top_k, int) or top_k is None:
            postprocess_params["top_k"] = top_k
            postprocess_params["_legacy"] = False
        elif return_all_scores is not None:
            warnings.warn(
                "`return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of"
                " `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.",
                UserWarning,
            )
            if return_all_scores:
                postprocess_params["top_k"] = None
            else:
                postprocess_params["top_k"] = 1

        if isinstance(function_to_apply, str):
            function_to_apply = ClassificationFunction[function_to_apply.upper()]

        if function_to_apply is not None:
            postprocess_params["function_to_apply"] = function_to_apply
        return preprocess_params, {}, postprocess_params

    def __call__(self, *args, **kwargs):
        """
        Classify the text(s) given as inputs.

        Args:
            args (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`):
                One or several texts to classify. In order to use text pairs for your classification, you can send a
                dictionary containing `{"text", "text_pair"}` keys, or a list of those.
            top_k (`int`, *optional*, defaults to `1`):
                How many results to return.
            function_to_apply (`str`, *optional*, defaults to `"default"`):
                The function to apply to the model outputs in order to retrieve the scores. Accepts four different
                values:

                If this argument is not specified, then it will apply the following functions according to the number
                of labels:

                - If the model has a single label, will apply the sigmoid function on the output.
                - If the model has several labels, will apply the softmax function on the output.

                Possible values are:

                - `"sigmoid"`: Applies the sigmoid function on the output.
                - `"softmax"`: Applies the softmax function on the output.
                - `"none"`: Does not apply any function on the output.

        Return:
            A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:

            - **label** (`str`) -- The label predicted.
            - **score** (`float`) -- The corresponding probability.

            If `top_k` is used, one such dictionary is returned per label.
        """
        result = super().__call__(*args, **kwargs)
        # TODO try and retrieve it in a nicer way from _sanitize_parameters.
        _legacy = "top_k" not in kwargs
        if isinstance(args[0], str) and _legacy:
            # This pipeline is odd, and return a list when single item is run
            return [result]
        else:
            return result

    def preprocess(self, inputs, **tokenizer_kwargs) -&gt; Dict[str, GenericTensor]:
        return_tensors = self.framework
        if isinstance(inputs, dict):
            return self.tokenizer(**inputs, return_tensors=return_tensors, **tokenizer_kwargs)
        elif isinstance(inputs, list) and len(inputs) == 1 and isinstance(inputs[0], list) and len(inputs[0]) == 2:
            # It used to be valid to use a list of list of list for text pairs, keeping this path for BC
            return self.tokenizer(
                text=inputs[0][0], text_pair=inputs[0][1], return_tensors=return_tensors, **tokenizer_kwargs
            )
        elif isinstance(inputs, list):
            # This is likely an invalid usage of the pipeline attempting to pass text pairs.
            raise ValueError(
                "The pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a"
                ' dictionary `{"text": "My text", "text_pair": "My pair"}` in order to send a text pair.'
            )
        return self.tokenizer(inputs, return_tensors=return_tensors, **tokenizer_kwargs)

    def _forward(self, model_inputs):
        # `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported
        model_forward = self.model.forward if self.framework == "pt" else self.model.call
        if "use_cache" in inspect.signature(model_forward).parameters.keys():
            model_inputs["use_cache"] = False
        return self.model(**model_inputs)

    def postprocess(self, model_outputs, function_to_apply=None, top_k=1, _legacy=True):
        # `_legacy` is used to determine if we're running the naked pipeline and in backward
        # compatibility mode, or if running the pipeline with `pipeline(..., top_k=1)` we're running
        # the more natural result containing the list.
        # Default value before `set_parameters`
        if function_to_apply is None:
            if self.model.config.problem_type == "multi_label_classification" or self.model.config.num_labels == 1:
                function_to_apply = ClassificationFunction.SIGMOID
            elif self.model.config.problem_type == "single_label_classification" or self.model.config.num_labels &gt; 1:
                function_to_apply = ClassificationFunction.SOFTMAX
            elif hasattr(self.model.config, "function_to_apply") and function_to_apply is None:
                function_to_apply = self.model.config.function_to_apply
            else:
                function_to_apply = ClassificationFunction.NONE

        outputs = model_outputs["logits"][0]
        outputs = outputs.numpy()

        if function_to_apply == ClassificationFunction.SIGMOID:
            scores = sigmoid(outputs)
        elif function_to_apply == ClassificationFunction.SOFTMAX:
            scores = softmax(outputs)
        elif function_to_apply == ClassificationFunction.NONE:
            scores = outputs
        else:
            raise ValueError(f"Unrecognized `function_to_apply` argument: {function_to_apply}")

        if top_k == 1 and _legacy:
            return {"label": self.model.config.id2label[scores.argmax().item()], "score": scores.max().item()}

        dict_scores = [
            {"label": self.model.config.id2label[i], "score": score.item()} for i, score in enumerate(scores)
        ]
        if not _legacy:
            dict_scores.sort(key=lambda x: x["score"], reverse=True)
            if top_k is not None:
                dict_scores = dict_scores[:top_k]
        return dict_scores
Call docstring:
Classify the text(s) given as inputs.

Args:
    args (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`):
        One or several texts to classify. In order to use text pairs for your classification, you can send a
        dictionary containing `{"text", "text_pair"}` keys, or a list of those.
    top_k (`int`, *optional*, defaults to `1`):
        How many results to return.
    function_to_apply (`str`, *optional*, defaults to `"default"`):
        The function to apply to the model outputs in order to retrieve the scores. Accepts four different
        values:

        If this argument is not specified, then it will apply the following functions according to the number
        of labels:

        - If the model has a single label, will apply the sigmoid function on the output.
        - If the model has several labels, will apply the softmax function on the output.

        Possible values are:

        - `"sigmoid"`: Applies the sigmoid function on the output.
        - `"softmax"`: Applies the softmax function on the output.
        - `"none"`: Does not apply any function on the output.

Return:
    A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:

    - **label** (`str`) -- The label predicted.
    - **score** (`float`) -- The corresponding probability.

    If `top_k` is used, one such dictionary is returned per label.</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>classifier.forward <span class="co"># what actually runs the inputs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;bound method Pipeline.forward of &lt;transformers.pipelines.text_classification.TextClassificationPipeline object&gt;&gt;</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>classifier.forward??</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Signature: classifier.forward(model_inputs, **forward_params)
Docstring: &lt;no docstring&gt;
Source:   
    def forward(self, model_inputs, **forward_params):
        with self.device_placement():
            if self.framework == "tf":
                model_inputs["training"] = False
                model_outputs = self._forward(model_inputs, **forward_params)
            elif self.framework == "pt":
                inference_context = self.get_inference_context()
                with inference_context():
                    model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)
                    model_outputs = self._forward(model_inputs, **forward_params)
                    model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device("cpu"))
            else:
                raise ValueError(f"Framework {self.framework} is not supported")
        return model_outputs
File:      ~/mambaforge/envs/llm_base/lib/python3.11/site-packages/transformers/pipelines/base.py
Type:      method</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>classifier._forward??</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Signature: classifier._forward(model_inputs)
Docstring:
_forward will receive the prepared dictionary from `preprocess` and run it on the model. This method might
involve the GPU or the CPU and should be agnostic to it. Isolating this function is the reason for `preprocess`
and `postprocess` to exist, so that the hot path, this method generally can run as fast as possible.

It is not meant to be called directly, `forward` is preferred. It is basically the same but contains additional
code surrounding `_forward` making sure tensors and models are on the same device, disabling the training part
of the code (leading to faster inference).
Source:   
    def _forward(self, model_inputs):
        # `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported
        model_forward = self.model.forward if self.framework == "pt" else self.model.call
        if "use_cache" in inspect.signature(model_forward).parameters.keys():
            model_inputs["use_cache"] = False
        return self.model(**model_inputs)
File:      ~/mambaforge/envs/llm_base/lib/python3.11/site-packages/transformers/pipelines/text_classification.py
Type:      method</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>classifier.model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>DistilBertForSequenceClassification(
  (distilbert): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0-5): 6 x TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>Summary: <code>?</code> and <code>??</code> are very powerful and useful inspection tools for Jupyter notebooks.</p>
</blockquote>
</section>
<section id="peeking-inside-the-pipeline" class="level1">
<h1>Peeking inside the <code>pipeline</code></h1>
<p>We can see the pipeline loaded the model.</p>
<p>It then handled the three key pieces (Config, Preprocess, Model) underneath the hood. What exactly is <code>pipeline</code> doing?</p>
<p>Letâ€™s build or own pipeline from scratch, stepping one small level below the abstraction. To do this, we will create each of the key pieces manually.</p>
<section id="config-class" class="level3">
<h3 class="anchored" data-anchor-id="config-class">Config class</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DistilBertConfig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="preprocessor-class" class="level3">
<h3 class="anchored" data-anchor-id="preprocessor-class">Preprocessor class</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DistilBertTokenizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-class" class="level3">
<h3 class="anchored" data-anchor-id="model-class">Model class</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># from transformers import DistilBertModel</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> DistilBertForSequenceClassification</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can use the modelâ€™s name from up above and build each piece ourselves. HuggingFace uses the <code>from_pretrained</code> method to make this quick and easy.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the model we are using</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">'distilbert-base-uncased-finetuned-sst-2-english'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># creating the config</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> DistilBertConfig.from_pretrained(model_name)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># creating the preprocessor </span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> DistilBertTokenizer.from_pretrained(model_name)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co"># creating the model</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DistilBertForSequenceClassification.from_pretrained(model_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next we build a simple pipeline with these manual pieces.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess(text: <span class="bu">str</span>):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Sends `text` through the LLM's tokenizer.  </span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co">    The tokenizers turns words and characters into special inputs for the LLM.</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    tokenized_inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenized_inputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"I love Fractal! I'm so glad it's not a cult!"</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>preprocess(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>{'input_ids': tensor([[  101,  1045,  2293, 25312, 25572,  2140,   999,  1045,  1005,  1049,
          2061,  5580,  2009,  1005,  1055,  2025,  1037,  8754,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(text):</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co">    First we preprocess the `text` into tokens.</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Then we send the `token_inputs` to the model.</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    token_inputs <span class="op">=</span> preprocess(text)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(<span class="op">**</span>token_inputs)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> forward(text)<span class="op">;</span> outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>SequenceClassifierOutput(loss=None, logits=tensor([[-3.3825,  3.5515]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>config.id2label[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'POSITIVE'</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>outputs.logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-3.3825,  3.5515]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_parameters(model):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="ss">f"</span><span class="sc">{</span>count_parameters(model)<span class="sc">:,}</span><span class="ss">"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>'66,955,010'</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_outputs(outs):</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Here is where HuggingFace does the most for us via `pipeline`.  </span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># grab the raw "scores" that from the model for Positive and Negative labels</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> outs.logits</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># find the strongest label score, aka the model's decision</span></span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>    pred_idx <span class="op">=</span> logits.argmax(<span class="dv">1</span>).item()</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use the `config` object to find the class label</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>    pred_label <span class="op">=</span> config.id2label[pred_idx]  </span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the human-readable number for the score</span></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>    pred_score <span class="op">=</span> logits.softmax(<span class="op">-</span><span class="dv">1</span>)[:, pred_idx].item()</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">'label'</span>: pred_label,</span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">'score'</span>: pred_score, </span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a>    }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_pipeline(text):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    model_outs <span class="op">=</span> forward(text)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> process_outputs(model_outs)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> preds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Letâ€™s call this pipeline on the same example text from before.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"We are very happy to show you the ðŸ¤— Transformers library."</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>simple_pipeline(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>{'label': 'POSITIVE', 'score': 0.9997795224189758}</code></pre>
</div>
</div>
</section>
</section>
<section id="more-hf-magic" class="level1">
<h1>More HF magic</h1>
<p><code>Auto</code> classes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoConfig</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"finiteautomata/bertweet-base-sentiment-analysis"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> AutoConfig.from_pretrained(model_name)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(model_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 949/949 [00:00&lt;00:00, 3.91MB/s]
Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:00&lt;00:00, 1.30MB/s]
Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 843k/843k [00:00&lt;00:00, 5.35MB/s]
Downloading (â€¦)solve/main/bpe.codes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.08M/1.08M [00:00&lt;00:00, 17.1MB/s]
Downloading (â€¦)in/added_tokens.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22.0/22.0 [00:00&lt;00:00, 103kB/s]
Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 167/167 [00:00&lt;00:00, 717kB/s]
emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0
Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 540M/540M [00:30&lt;00:00, 17.6MB/s] </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>simple_pipeline(text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>{'label': 'POS', 'score': 0.9929807186126709}</code></pre>
</div>
</div>
<section id="more-hugging-face-magic" class="level2">
<h2 class="anchored" data-anchor-id="more-hugging-face-magic">More Hugging Face Magic</h2>
<p><code>Auto</code> classes.</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoConfig</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"nghuyong/ernie-3.0-nano-zh"</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>tweet_classifier_model <span class="op">=</span> <span class="st">"vinai/bertweet-base"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>bertweet <span class="op">=</span> AutoModel.from_pretrained(tweet_classifier_model)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="co"># For transformers v4.x+:</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(tweet_classifier_model, use_fast<span class="op">=</span><span class="va">False</span>)<span class="co"># It seems like the torch library is not imported. Let's import it.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</code></pre>
</div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># INPUT TWEET IS ALREADY NORMALIZED!</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>line <span class="op">=</span> <span class="st">"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:"</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> torch.tensor([tokenizer.encode(line)])</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> bertweet(input_ids)  <span class="co"># Models outputs are now tuples</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>features</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0261,  0.2147,  0.1159,  ...,  0.0314,  0.0336, -0.1419],
         [ 0.0986, -0.0205,  0.2265,  ...,  0.0828, -0.2993,  0.4767],
         [-0.1011,  0.2133, -0.2283,  ...,  0.0797, -0.1762, -0.2632],
         ...,
         [-0.3448, -0.2996, -0.2430,  ..., -0.3354,  0.2429,  0.3029],
         [-0.5279, -0.2429,  0.0758,  ..., -0.1733,  0.0389,  0.0513],
         [-0.0425,  0.2355,  0.1219,  ...,  0.0018,  0.0732, -0.1305]]]), pooler_output=tensor([[ 2.1684e-01, -1.6747e-01, -3.9913e-02, -1.6048e-01,  1.2537e-01,
         -7.5435e-02,  2.1886e-01, -1.3510e-01,  2.0169e-01, -2.1097e-01,
         -9.4783e-02, -5.3385e-02, -2.2015e-01,  1.3985e-02,  1.5082e-01,
         -6.2458e-02, -1.4717e-01, -6.7478e-02,  2.4149e-02,  1.7152e-01,
         -1.1483e-01, -1.7264e-01,  2.9670e-01, -1.6612e-02, -2.3634e-02,
          3.5211e-02, -1.6087e-01, -3.7129e-02,  4.9395e-02, -5.8116e-02,
         -5.6990e-02, -1.5559e-01,  1.6120e-01,  5.6176e-02,  1.7266e-01,
         -5.9471e-02,  2.1309e-01,  1.2466e-01, -1.2592e-01,  9.7800e-02,
         -1.5980e-01, -4.7554e-02,  1.1069e-01,  2.2540e-02, -7.0860e-02,
          3.4330e-02, -1.3061e-01,  3.6223e-02,  3.8687e-01, -4.9398e-02,
         -6.2529e-03,  1.1077e-01, -6.5433e-02, -1.0200e-02, -5.4492e-03,
          1.2476e-01, -5.6954e-03,  6.2194e-02, -2.2040e-01,  2.3035e-01,
          4.0335e-02,  4.2737e-01, -2.5728e-01, -4.2542e-01, -1.2840e-02,
         -1.3663e-01, -4.3941e-03,  1.1903e-01,  4.6278e-01,  2.3251e-01,
          2.7170e-01,  1.0339e-01, -2.4488e-01, -1.2348e-02, -9.7118e-02,
         -1.6682e-01,  6.5080e-02, -2.9772e-02,  1.3511e-01,  2.4833e-02,
         -2.9946e-02, -3.5176e-01,  2.3219e-01, -9.5713e-02, -2.5918e-01,
          2.1088e-01,  1.0469e-01,  6.2564e-02, -6.9195e-02,  2.1744e-01,
         -1.8241e-01, -7.7701e-02, -1.3959e-01, -2.1408e-01, -1.0395e-01,
         -2.8095e-02,  2.9942e-04,  2.3257e-01, -1.2731e-01, -1.7985e-01,
         -1.8892e-01,  8.1731e-02, -3.7738e-03,  6.8216e-03,  4.1659e-03,
         -8.9084e-02, -3.5341e-02,  2.2507e-01,  2.8873e-02, -7.7968e-02,
         -2.0763e-02, -1.6699e-01, -2.2962e-01, -1.6147e-01,  1.9951e-01,
         -2.0290e-02, -3.0334e-01,  2.8734e-01,  2.3261e-01,  7.0353e-02,
         -3.4001e-02,  2.0040e-01,  1.5577e-02,  3.0432e-01,  5.7032e-02,
         -3.4262e-02,  6.6559e-02, -6.2958e-02,  1.2308e-01, -2.3498e-01,
          5.8435e-02, -1.7375e-02,  1.5279e-01, -1.6965e-01,  1.1444e-01,
         -1.9653e-02,  1.4225e-01,  8.3226e-02,  1.7470e-01,  1.6492e-01,
          2.9033e-03, -2.9698e-01, -1.7966e-02,  3.3161e-01,  2.6770e-03,
          3.0237e-01, -1.2641e-01,  1.4140e-01, -9.1238e-02, -4.3958e-02,
         -2.0406e-03, -1.0386e-01, -1.9405e-01,  1.6783e-01, -4.5707e-02,
         -8.6509e-02,  4.4101e-04,  6.7585e-02,  5.1672e-02, -9.1920e-02,
          2.1051e-01, -2.8929e-01,  1.1323e-02, -1.5257e-01,  7.3889e-02,
         -7.6998e-02, -1.5864e-01, -5.7710e-02, -1.4706e-01, -2.0447e-02,
         -1.4777e-01, -1.7777e-01, -6.2066e-01, -2.8002e-01,  2.0328e-01,
          1.6720e-01, -1.3829e-01,  9.3785e-02,  1.9930e-01,  6.8548e-02,
         -8.3035e-03,  1.0518e-01, -2.2478e-01,  3.6735e-02, -1.4841e-01,
          2.2288e-01, -9.9642e-02, -9.6726e-02,  6.0620e-02,  3.3674e-02,
          7.1847e-02, -1.7697e-02,  2.5305e-01,  1.8194e-01, -3.8007e-01,
         -1.7328e-01,  1.5387e-01, -1.9752e-01,  5.8195e-02,  1.4141e-01,
          1.7599e-01,  9.9589e-02, -1.7539e-01,  3.9723e-02,  5.7362e-02,
         -6.0153e-03, -7.4778e-02, -3.9814e-02,  4.2532e-03, -6.1438e-03,
         -2.7549e-01, -1.7463e-01, -1.0434e-01, -2.8234e-01, -1.1606e-01,
          2.2520e-01, -2.6030e-01, -5.6626e-02, -4.5074e-01,  2.0554e-01,
          2.4415e-01, -2.4389e-01,  1.4430e-01,  2.7835e-02, -8.4409e-02,
         -5.5983e-04,  4.9588e-01,  7.3265e-02, -1.3965e-01, -3.0254e-01,
         -3.4468e-02, -8.2590e-02,  1.2788e-01, -1.6157e-02,  1.7172e-01,
          6.1288e-01,  1.5453e-01,  9.4416e-02, -1.1867e-01, -2.8974e-02,
         -2.2069e-01, -1.8120e-01,  9.5515e-02, -1.8660e-01,  3.1271e-02,
          3.6478e-02,  4.3989e-01,  5.9990e-02, -2.1676e-01,  9.2674e-02,
         -1.1439e-01, -2.4943e-01, -2.9657e-01,  1.3715e-01, -5.5204e-02,
          8.1869e-02, -2.5680e-03,  4.9731e-03,  2.9959e-01,  1.8850e-01,
          9.5238e-02,  1.7245e-01, -2.8302e-01, -1.0291e-01, -1.3128e-01,
         -7.4131e-02,  6.4492e-02, -2.6746e-01, -7.7197e-02, -1.6304e-01,
         -2.2387e-01, -1.2678e-01,  6.6664e-02, -2.5839e-02, -8.8316e-02,
         -2.3827e-01, -6.9970e-03,  1.8078e-01, -6.2181e-02, -3.8105e-01,
          1.2478e-02,  2.3644e-01, -2.2813e-01, -6.5327e-02, -4.2093e-02,
         -4.9957e-02, -1.6963e-02,  2.8927e-01,  1.2748e-01, -1.6388e-02,
         -1.4192e-01, -1.0516e-01, -7.7114e-03,  3.0210e-01,  7.8765e-02,
         -2.9639e-01,  1.6735e-01,  1.0467e-02,  2.1078e-01,  7.6494e-02,
         -1.1333e-01, -1.7217e-01,  1.1373e-01, -2.2313e-01, -1.2148e-01,
         -1.9967e-01, -1.0848e-01, -1.8277e-03,  1.9473e-01,  1.9243e-01,
          1.7690e-01, -2.0756e-02,  2.1948e-01,  2.6942e-01, -6.2986e-03,
          1.4327e-01,  1.1615e-01,  2.0209e-01,  3.8537e-02, -4.6930e-02,
         -7.8353e-02, -1.7991e-01, -1.6937e-01, -4.8626e-02,  2.6269e-02,
          1.7385e-02, -1.7453e-01,  2.6845e-02, -5.6886e-02,  2.7073e-01,
          2.8761e-01, -1.0229e-01,  1.4652e-01, -8.4829e-02, -1.5249e-01,
         -1.6665e-01,  1.6805e-01,  7.7209e-02,  5.8676e-02,  4.0058e-02,
         -1.5571e-01, -1.2718e-01,  3.1281e-01, -9.1534e-03,  1.3576e-01,
          3.9473e-01,  2.4109e-02, -3.8084e-02, -1.4643e-01,  2.8824e-02,
         -3.2364e-02, -1.4315e-01,  3.7451e-02, -4.4504e-02,  1.5163e-01,
         -2.8780e-01,  1.6395e-01,  1.3348e-01, -2.2630e-01,  7.0919e-02,
         -2.1954e-01,  2.5653e-02, -8.2631e-02, -1.2159e-01, -1.5556e-01,
          7.7520e-02, -1.2948e-01, -2.3807e-01,  2.7228e-01, -4.2869e-02,
         -7.8903e-03,  8.9008e-02,  3.2700e-01, -1.7110e-01, -1.3331e-01,
          2.4333e-02,  2.5774e-02, -3.7812e-01, -1.7119e-01, -7.4228e-02,
          1.4139e-01, -1.0524e-01, -2.4698e-01,  1.0323e-01, -1.0664e-01,
          9.5311e-02, -3.6480e-01,  1.9375e-01,  2.4034e-01, -1.3027e-01,
          2.6347e-01, -2.1574e-02,  7.0733e-03, -4.5205e-01,  1.0983e-01,
          7.4580e-02,  1.9370e-01,  5.6969e-02,  3.0360e-01, -1.0367e-01,
         -6.9653e-02, -2.0863e-01,  1.1197e-01, -8.3405e-02, -4.3823e-02,
         -1.0581e-01, -2.6405e-03,  1.2920e-01,  7.2932e-02, -8.2181e-03,
         -1.5625e-01,  2.5517e-01, -3.2143e-01, -1.0550e-01, -8.6524e-02,
          1.3654e-01,  2.1519e-01,  3.3233e-02,  1.1481e-01, -2.7192e-01,
          1.5108e-01, -1.2987e-01,  5.7868e-02,  4.9990e-02, -5.4458e-02,
          2.7341e-02, -1.0211e-01, -7.1317e-02, -1.0423e-01, -4.7862e-02,
         -1.6483e-02, -1.6297e-01, -1.1742e-01, -1.3970e-02, -9.0472e-02,
          3.6632e-02,  5.2472e-02,  1.0371e-01,  5.5842e-02, -1.8505e-01,
         -1.7271e-01, -3.4367e-01,  2.3774e-01, -2.4568e-01, -6.8554e-02,
         -1.1043e-01, -2.1823e-01, -3.3877e-01, -1.8825e-01, -1.3074e-01,
          2.0080e-01, -4.3499e-02, -1.1833e-03, -1.0494e-02,  9.8839e-02,
         -9.8610e-02,  1.0916e-01, -1.2520e-01,  2.2091e-01,  1.8034e-01,
          1.3691e-01,  5.8596e-02,  9.1620e-02,  2.5686e-01, -1.6616e-01,
         -2.1491e-01,  1.5801e-01, -6.9085e-02, -1.1327e-01,  2.2038e-01,
          4.0991e-02, -1.2667e-01, -2.1626e-01,  9.1708e-02, -2.1187e-01,
          3.8479e-02,  1.3006e-02,  2.8096e-01,  5.8074e-02,  6.4363e-02,
         -1.9326e-01,  5.0338e-02, -1.6850e-01,  2.5039e-01, -1.2964e-01,
         -4.5641e-02, -1.9423e-01, -2.0506e-01,  2.7561e-01, -2.1644e-01,
          1.3510e-01, -1.5607e-01, -1.9618e-01,  3.6909e-01,  1.6661e-02,
         -2.7980e-01,  1.6374e-01,  9.8737e-02, -4.2289e-02, -5.9343e-03,
         -2.3921e-01, -9.5844e-02,  7.9161e-02,  4.8087e-02, -1.5758e-01,
         -3.2436e-01, -2.1874e-01,  9.2943e-02, -1.7660e-01,  3.2471e-01,
          4.1639e-02, -8.4214e-02, -1.1156e-01,  1.7573e-01, -1.4107e-01,
          9.6379e-02, -8.0719e-02, -2.1352e-01,  1.9738e-01, -7.8069e-04,
         -1.4115e-01, -1.4976e-01, -1.0283e-01,  2.7926e-01,  2.2314e-01,
          2.6503e-01, -2.9287e-01,  5.3031e-03, -1.6676e-01, -1.7062e-01,
          1.6481e-01, -6.6617e-02,  1.3794e-01,  1.2838e-01,  1.7721e-01,
         -3.6212e-01,  1.9786e-03, -4.0427e-02, -1.4486e-01, -9.7480e-02,
         -1.8925e-01,  4.2498e-01, -5.7728e-02, -4.3533e-02,  1.7206e-02,
          1.3141e-01, -2.7031e-02, -1.0911e-01,  3.1601e-01,  5.3195e-02,
          2.1432e-01, -1.2668e-01,  8.3884e-02, -3.1696e-01, -1.4914e-01,
         -3.0770e-01,  5.2150e-01, -1.0679e-01,  1.0872e-01, -3.0105e-02,
         -1.8292e-01,  2.4955e-01,  4.8210e-02,  6.5361e-02, -8.1284e-02,
         -7.3967e-02,  2.1774e-01, -2.5905e-01, -9.9748e-02,  2.2861e-01,
          2.0665e-01, -6.5433e-02, -1.4660e-01, -2.7731e-02, -3.7495e-01,
         -3.3564e-01,  2.2604e-01,  1.2786e-01, -1.1509e-01, -3.2010e-01,
          5.0551e-02, -5.2174e-02, -6.8957e-02, -3.2839e-01, -1.4177e-01,
          1.2305e-01,  5.3620e-02, -2.2995e-01, -1.0491e-01, -7.8523e-02,
         -5.8274e-02, -3.0538e-01,  2.1489e-01,  1.6900e-01,  1.0598e-01,
          1.4898e-03, -8.4836e-02, -2.2781e-01,  6.4068e-02,  6.7883e-02,
          5.4033e-01,  7.4442e-02,  2.1099e-01, -2.4285e-01, -9.5264e-02,
         -7.1959e-02,  7.2113e-02,  1.9963e-01,  7.2252e-02, -1.8008e-01,
          1.1053e-02,  7.1147e-03,  1.3077e-01, -1.1621e-01,  1.3477e-01,
          1.8277e-01, -1.2061e-01,  2.5727e-01, -4.8499e-02, -7.5046e-02,
         -1.1551e-01,  5.9584e-02,  1.1659e-01, -2.9556e-01,  3.4345e-02,
          2.3334e-01, -3.8725e-02, -4.8951e-02,  1.2001e-01, -1.2271e-01,
          1.2176e-01,  1.7248e-01,  1.7510e-01, -1.0586e-01,  1.4408e-01,
          4.6973e-02, -1.9982e-01, -3.1675e-01,  1.8192e-01, -4.9504e-02,
         -1.1049e-01,  5.1719e-02, -3.8881e-01, -3.7701e-01, -2.3664e-01,
          6.1054e-02, -1.6371e-01, -1.8980e-02, -1.0350e-02, -1.8236e-02,
         -3.1771e-01,  1.7057e-02, -6.7176e-02,  3.0066e-02,  1.2955e-01,
          2.5587e-02, -1.3062e-02,  1.5314e-01,  1.2715e-01,  2.5056e-01,
         -2.6158e-01, -7.3815e-02, -2.8139e-01, -6.8489e-02, -2.4890e-01,
         -1.0459e-01,  2.1866e-01, -1.3652e-02,  2.2082e-01,  1.4452e-01,
          1.2156e-01, -4.2408e-03,  2.5653e-01, -1.4201e-02, -1.3602e-01,
          2.4607e-02, -2.4028e-01,  9.0389e-02, -2.0443e-01, -2.6975e-01,
          2.9034e-01, -8.2160e-02, -4.7939e-02,  2.1328e-01,  4.1331e-02,
         -1.3397e-01, -5.8570e-02, -1.4730e-01,  2.0228e-01,  2.1142e-01,
          1.9471e-01, -9.2305e-02,  2.7115e-02,  2.2871e-02, -2.3399e-01,
         -6.6573e-02, -3.1556e-02, -6.7305e-02, -9.1438e-02, -2.6135e-01,
         -7.6051e-02, -1.3300e-01,  2.3206e-01,  2.1293e-01, -4.6348e-02,
         -1.7028e-01, -3.1770e-02,  3.3346e-03, -7.7843e-03,  2.5576e-01,
          4.3425e-02,  1.1490e-02, -9.8304e-02, -2.2572e-01, -5.6736e-02,
          2.4170e-01, -8.3043e-02, -3.2750e-01, -1.8894e-01, -7.3210e-02,
          2.4887e-02, -5.6985e-02,  3.2579e-01, -7.0417e-02,  5.7850e-02,
         -5.1130e-02, -6.9433e-02, -1.9004e-01,  1.9591e-02, -1.7293e-01,
         -1.5911e-01,  9.1796e-02,  3.4482e-01, -8.8054e-03,  9.0982e-02,
         -2.2528e-02, -1.4741e-01,  5.4736e-02, -2.4447e-01, -1.3338e-01,
         -1.2902e-01, -1.2377e-01, -2.4834e-01, -2.0397e-02, -4.5321e-03,
          1.2454e-01, -4.7694e-02,  1.7699e-01, -3.2227e-01,  2.7142e-01,
         -1.2625e-01, -2.3601e-01,  9.6091e-02,  3.3450e-01, -1.4860e-01,
         -1.3938e-01,  1.1786e-01,  2.0373e-02, -1.1923e-01, -2.8852e-01,
         -2.2158e-01,  1.6883e-01,  1.2095e-01,  3.1874e-01, -3.6485e-01,
          1.2620e-02, -6.6986e-02, -1.9560e-01]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)</code></pre>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>