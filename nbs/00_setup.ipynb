{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setups\n",
    "\n",
    "> Getting your computer ready for the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lesson_0.setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "\n",
    "The goal of this course is to build an AI Agent by fine-tuning a Large Language Model (LLM) on documents chosen by each student. What exactly do we mean by Agent, and why would we focus on a simple one?  \n",
    "\n",
    "To give some context, there is a lot of talk and hype around Agents at the moment. Folks are looking towards a future where we all have personalized AI assistants, aka Agents, at our fingertips. These Agents will make our lives better both in the day to day and in the long term. They are the AI assistants of Science Fiction made material. Agents like TARS from Interstellar, Jarvis from Iron Man, HAL 9000 from Space Odyssey, Samantha from Her, etc.  \n",
    "\n",
    "For as fast as the progress in AI and LLMs has been, Agents as powerful as those listed ones are still a ways off. It's hard if not impossible to predict the exact timelines. Suffice it to say that these Agents won't be here anytime \"soon\". But, barring some force majeure, they *will* exist at some point.   \n",
    "\n",
    "The gap, then, is that folks are talking about (and promising) these advanced Agent capabilities, while on the ground we are still dealing with LLM hallucinations and prohibitive compute requirements.  \n",
    "\n",
    "Where does that leave us? Well, as the recent announcement from OpenAI showed:  (BROCKMANS TWEET)[link], a GPT-3.5 model fine-tuned on small, clean datasets can actually surpass GPT-4 on certain tasks. *That's* what we are aiming for. In other words, we already have the ability to develop powerful tools by fine-tuning LLMs on small, clean datasets.  \n",
    "\n",
    "So we will neither be building a simple chatbot, nor will we be building the Jarvis of Iron Man. We will land somehwere in the middle.  If it helps, try thinking about this simple Agent as an Intelligent Rubber Duck. If you're not familiar with the concept, a Rubber Duck is anything (actual yellow rubber duckie optional) that you keep around during work, and that you talk to about your work. It's like a physical tool for thought, since it's often very illuminating to speak out loud the swarm of thoughts and ideas in our head.  \n",
    "\n",
    "Our simple Agent will be a Rubber Duck that can speak back at you. When you ask it a question about your work, it will respond given what it knows about the project as a whole. And when you are simply verbalizing a thought to untangle it, the Agent will give you some feedback or suggest other approaches. If we can be so bold: our Agent will be a Jarvis-lite, laser-focused on a narrow scope. Then, as both the tools and tech progresses, we'll have everything we need to unlock even more capabilities from our Intelligent Rubber Duck. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Summary: In this course we will fine-tune an LLM on a small, clean dataset of our choosing to build an Intelligent Rubber Duck that can help us work or create better.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things we need for the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fully use a current, open source LLM, we first need to setup a proper `programming environmnet`` on our computer. This `environment` is a computing ecosystem with all of the software libraries and packages we need to run the LLMs.  \n",
    "\n",
    "Note that setting up this environment is often one of the most time-consuming and challenging tasks when it comes to Machine Learning. There is no silver bullet or universal solution, as you will see by the dozens of tools that folks have come up with to tackle this problem [xkcd comic about competing standards].  \n",
    "\n",
    "The main point here is that setting up the environment can be annoying. Even straight up painful. It's ok to feel lost and struggle with it. Please take some comfort in the fact that once we have the environment, many of the downstream tasks will feel easy by comparison!  \n",
    "\n",
    "So what makes building this environment so challenging? And why do we need it in the first place? \n",
    "\n",
    "\n",
    "#### Silent failures  \n",
    "\n",
    "LLMs, and Machine Learning models more generally, often fail in different ways than other, standard software fails. For instance, some classic bugs in a regular piece of software include: type mismatches, syntax errors, compilation errors, etc. In other words these failures stem from a clearly *wrong* operation or step (aka a bug) that snuck into the code. We wanted the computer to do `X`, but we told it to do `Y` instead by accident.\n",
    "\n",
    "In contrast ML models often have \"silent\" failure modes. There is no syntax or compilation error - the program runs and completes fine. But, there is *something* still wrong in the code: adding where we should have subtracted, grabbing the wrong element from a list, or using the wrong mathematical function. There is no type checker or compiler that would or even could catch these errors.  \n",
    "\n",
    "The fix for the silent failures above is clear: careful inspection of the code, monitoring and validation of the outputs, and clarity in both the algorithms and models we are using. \n",
    "\n",
    "There is another, unforunate kind of silent failure: `version` mismatches. These failures are immune to our careful debugging eyes or logical checks. Version faliures happen when we use a different version of a programming library than the version originally used by the model. As the software libraries we rely on are frequently updated, both subtle and major changes in their internals can affect the output of a model.  \n",
    "\n",
    "These version failures are the main reason for being consistent and disciplined with our model's programming environment. We already have to stay vigilant for any logical silent failures. A good environment setup keeps us focused on the important conceptual part of our model instead of getting bogged down with tracking and matching software versions. \n",
    "\n",
    "\n",
    "#### Looking forward with our environment  \n",
    "\n",
    "There is a nice benefit to spending this much time and effort up front on our environment.  \n",
    "\n",
    "We will not only have a specialized environment to run and fine-tune a single LLM. We'll have a springboard and setup to keep up with the state of the art in the field. To weave in groundbreaking improvements as they're release. To work in the latest and greatest models. The LLM world is our oyster, and our environment skills the grain of sand soon made pearls. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing what we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Environment Plan:\n",
    "  Mamba package manager for the base python version.\n",
    "     Makes it easier to install other OS and system-level libraries. \n",
    "     Can mix with homebrew as needed or preference\n",
    "\n",
    "  Pip for packages, mainly cause a few libraries need special commands. \n",
    "\n",
    "  `pip install -e .` is a good way to build packages from scratch when needed.\n",
    "\n",
    "   Three options for our environment:\n",
    "\n",
    "      Local\n",
    "         Mainly some command differences when using Mac vs. Linux\n",
    "         WSL on Windows \n",
    "\n",
    "      On the cloud\n",
    "         (Lambda Labs)[https://lambdalabs.com/]\n",
    "      \n",
    "      Docker images\n",
    "         If folks don't want to deal with the setup.  \n",
    "         Point and click, easy to customize and, eventually, distribute.\n",
    "\n",
    "\n",
    "\n",
    "Nbdev for writing and creating the content.\n",
    "  Notebook, export to file.\n",
    "  Publish via quarto to a website.\n",
    "     Github pages.\n",
    "  Linking to substack?\n",
    "  Explorative, iterative programming.\n",
    "     https://en.wikipedia.org/wiki/Literate_programming\n",
    "        URL: https://en.wikipedia.org/wiki/Literate_programming\n",
    "     \"â€¦a move away from writing computer programs in the manner and order imposed by the computer, and instead enables programmers to develop programs in the order demanded by the logic and flow of their thoughts.\" - Donald Knuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options and alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Setting up the mamba environment \n",
    "\n",
    "[Full instructions](https://github.com/conda-forge/miniforge#install)  \n",
    "\n",
    "\n",
    "## Mac Installation  \n",
    "\n",
    "First find out the name of your architecture. This will determine which script we download.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "# find your mac's architecture\n",
    "arch=$(uname) \n",
    "\n",
    "# download the appropriate script\n",
    "curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\n",
    "\n",
    "# install Mambaforge\n",
    "bash Mambaforge-$(uname)-$(uname -m).sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to download the file directly, grab it from here:  \n",
    "\n",
    "https://github.com/conda-forge/miniforge/releases/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
