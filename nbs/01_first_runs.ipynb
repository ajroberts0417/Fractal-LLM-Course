{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setups\n",
    "\n",
    "> Run a Large Language Model using the [HuggingFace `Transformers`](https://huggingface.co/docs/transformers/index) API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lesson_1.first_run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are good defaults for development.  \n",
    "\n",
    "The `autoreload` lines help load libraries on the fly, while they are changing. This works well with the editable install we created via `pip install -e .`  \n",
    "This means we can edit the source code directly and have the change reflected live in the notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a list of product review from our users. Now we want to find out whether those reviews were good or bad. It will take a lot of effort to manually go through and check each one. But, using an LLM, we can automatically get a label for a given product review. \n",
    "\n",
    "How would this be useful? We could use it to find the more negative reviews to see where our product needs improving. Or, we can look at the more positive ones to see what we're doing right.  \n",
    "\n",
    "The broader task in NLP of figuring out a statement's tone is called `Sentiment Analysis`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, a Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A HuggingFace model is based on 3 key pieces: \n",
    "1. Config file.  \n",
    "2. Preprocessor file.   \n",
    "3. Model file.   \n",
    "\n",
    "The HuggingFace API gives us a way of automatically using these pieces directly: the `pipeline`.  \n",
    "\n",
    "Let's get right it and create a Sentiment Analysis `pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 629/629 [00:00<00:00, 752kB/s]\n",
      "Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:21<00:00, 12.6MB/s] \n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00<00:00, 388kB/s]\n",
      "Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 5.65MB/s]\n"
     ]
    }
   ],
   "source": [
    "#| export \n",
    "\n",
    "# load in the pipeline object from huggingface\n",
    "from transformers import pipeline\n",
    "\n",
    "# create the sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the output message above that HuggingFace automatically picked a decent, default model for us since we didn't specify one. Specifically, it chose a [distilbert model](distilbert-base-uncased-finetuned-sst-2-english).  \n",
    "\n",
    "We will learn more about what exactly `distilbert` is and how it works later on. For now, think of it as a useful NLP genie who can tell us how it feels about a given sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "# example from the HuggingFace tutorial\n",
    "classifier(\"We are very happy to show you the ðŸ¤— Transformers library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n",
      "label: POSITIVE, with score: 0.999\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "# passing in several sentences at once, inside a python list\n",
    "results = classifier([\n",
    "    \"We are very happy to show you the ðŸ¤— Transformers library.\",\n",
    "    \"We hope you don't hate it.\",\n",
    "    \"I love Fractal! I'm so glad it's not a cult!\", \n",
    "])\n",
    "\n",
    "# print the output of each results\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the `classifier`, notebook style."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the `classifier`, exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x16d2fe690>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## showing the lookup's auto-complete\n",
    "# classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_batch_size',\n",
       " '_ensure_tensor_on_device',\n",
       " '_forward',\n",
       " '_forward_params',\n",
       " '_num_workers',\n",
       " '_postprocess_params',\n",
       " '_preprocess_params',\n",
       " '_sanitize_parameters',\n",
       " 'binary_output',\n",
       " 'call_count',\n",
       " 'check_model_type',\n",
       " 'default_input_names',\n",
       " 'device',\n",
       " 'device_placement',\n",
       " 'ensure_tensor_on_device',\n",
       " 'feature_extractor',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'function_to_apply',\n",
       " 'get_inference_context',\n",
       " 'get_iterator',\n",
       " 'image_processor',\n",
       " 'iterate',\n",
       " 'model',\n",
       " 'modelcard',\n",
       " 'postprocess',\n",
       " 'predict',\n",
       " 'preprocess',\n",
       " 'return_all_scores',\n",
       " 'run_multi',\n",
       " 'run_single',\n",
       " 'save_pretrained',\n",
       " 'task',\n",
       " 'tokenizer',\n",
       " 'torch_dtype',\n",
       " 'transform']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## viewing all of a class' methods and properties\n",
    "dir(classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebooks have powerful ways of inspecting and analyzing the code, as we're running it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x16d2fe690>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## refresher\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           TextClassificationPipeline\n",
      "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.text_classification.TextClassificationPipeline object at 0x16d2fe690>\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/llm_base/lib/python3.11/site-packages/transformers/pipelines/text_classification.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Text classification pipeline using any `ModelForSequenceClassification`. See the [sequence classification\n",
      "examples](../task_summary#sequence-classification) for more information.\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      ">>> from transformers import pipeline\n",
      "\n",
      ">>> classifier = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
      ">>> classifier(\"This movie is disgustingly good !\")\n",
      "[{'label': 'POSITIVE', 'score': 1.0}]\n",
      "\n",
      ">>> classifier(\"Director tried too much.\")\n",
      "[{'label': 'NEGATIVE', 'score': 0.996}]\n",
      "```\n",
      "\n",
      "Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      "\n",
      "This text classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      "`\"sentiment-analysis\"` (for classifying sequences according to positive or negative sentiments).\n",
      "\n",
      "If multiple classification labels are available (`model.config.num_labels >= 2`), the pipeline will run a softmax\n",
      "over the results. If there is a single label, the pipeline will run a sigmoid over the result.\n",
      "\n",
      "The models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See\n",
      "the up-to-date list of available models on\n",
      "[huggingface.co/models](https://huggingface.co/models?filter=text-classification).\n",
      "\n",
      "Arguments:\n",
      "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      "    tokenizer ([`PreTrainedTokenizer`]):\n",
      "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      "        [`PreTrainedTokenizer`].\n",
      "    modelcard (`str` or [`ModelCard`], *optional*):\n",
      "        Model card attributed to the model for this pipeline.\n",
      "    framework (`str`, *optional*):\n",
      "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      "        installed.\n",
      "\n",
      "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      "        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      "        provided.\n",
      "    task (`str`, defaults to `\"\"`):\n",
      "        A task-identifier for the pipeline.\n",
      "    num_workers (`int`, *optional*, defaults to 8):\n",
      "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      "        workers to be used.\n",
      "    batch_size (`int`, *optional*, defaults to 1):\n",
      "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      "        the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      "        pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      "    args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      "        Reference to the object in charge of parsing supplied pipeline parameters.\n",
      "    device (`int`, *optional*, defaults to -1):\n",
      "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      "        the associated CUDA device id. You can pass native `torch.device` or a `str` too.\n",
      "    binary_output (`bool`, *optional*, defaults to `False`):\n",
      "        Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.\n",
      "\n",
      "    return_all_scores (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to return all prediction scores or just the one of the predicted class.\n",
      "    function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      "        The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:\n",
      "\n",
      "        - `\"default\"`: if the model has a single label, will apply the sigmoid function on the output. If the model\n",
      "          has several labels, will apply the softmax function on the output.\n",
      "        - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      "        - `\"softmax\"`: Applies the softmax function on the output.\n",
      "        - `\"none\"`: Does not apply any function on the output.\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Classify the text(s) given as inputs.\n",
      "\n",
      "Args:\n",
      "    args (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`):\n",
      "        One or several texts to classify. In order to use text pairs for your classification, you can send a\n",
      "        dictionary containing `{\"text\", \"text_pair\"}` keys, or a list of those.\n",
      "    top_k (`int`, *optional*, defaults to `1`):\n",
      "        How many results to return.\n",
      "    function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      "        The function to apply to the model outputs in order to retrieve the scores. Accepts four different\n",
      "        values:\n",
      "\n",
      "        If this argument is not specified, then it will apply the following functions according to the number\n",
      "        of labels:\n",
      "\n",
      "        - If the model has a single label, will apply the sigmoid function on the output.\n",
      "        - If the model has several labels, will apply the softmax function on the output.\n",
      "\n",
      "        Possible values are:\n",
      "\n",
      "        - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      "        - `\"softmax\"`: Applies the softmax function on the output.\n",
      "        - `\"none\"`: Does not apply any function on the output.\n",
      "\n",
      "Return:\n",
      "    A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:\n",
      "\n",
      "    - **label** (`str`) -- The label predicted.\n",
      "    - **score** (`float`) -- The corresponding probability.\n",
      "\n",
      "    If `top_k` is used, one such dictionary is returned per label."
     ]
    }
   ],
   "source": [
    "## the power of asking questions\n",
    "classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           TextClassificationPipeline\n",
      "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.text_classification.TextClassificationPipeline object at 0x16d2fe690>\n",
      "\u001b[0;31mFile:\u001b[0m           ~/mambaforge/envs/llm_base/lib/python3.11/site-packages/transformers/pipelines/text_classification.py\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;34m@\u001b[0m\u001b[0madd_end_docstrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mPIPELINE_INIT_ARGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34mr\"\"\"\u001b[0m\n",
      "\u001b[0;34m        return_all_scores (`bool`, *optional*, defaults to `False`):\u001b[0m\n",
      "\u001b[0;34m            Whether to return all prediction scores or just the one of the predicted class.\u001b[0m\n",
      "\u001b[0;34m        function_to_apply (`str`, *optional*, defaults to `\"default\"`):\u001b[0m\n",
      "\u001b[0;34m            The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            - `\"default\"`: if the model has a single label, will apply the sigmoid function on the output. If the model\u001b[0m\n",
      "\u001b[0;34m              has several labels, will apply the softmax function on the output.\u001b[0m\n",
      "\u001b[0;34m            - `\"sigmoid\"`: Applies the sigmoid function on the output.\u001b[0m\n",
      "\u001b[0;34m            - `\"softmax\"`: Applies the softmax function on the output.\u001b[0m\n",
      "\u001b[0;34m            - `\"none\"`: Does not apply any function on the output.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mTextClassificationPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Text classification pipeline using any `ModelForSequenceClassification`. See the [sequence classification\u001b[0m\n",
      "\u001b[0;34m    examples](../task_summary#sequence-classification) for more information.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    >>> from transformers import pipeline\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> classifier = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")\u001b[0m\n",
      "\u001b[0;34m    >>> classifier(\"This movie is disgustingly good !\")\u001b[0m\n",
      "\u001b[0;34m    [{'label': 'POSITIVE', 'score': 1.0}]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> classifier(\"Director tried too much.\")\u001b[0m\n",
      "\u001b[0;34m    [{'label': 'NEGATIVE', 'score': 0.996}]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This text classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:\u001b[0m\n",
      "\u001b[0;34m    `\"sentiment-analysis\"` (for classifying sequences according to positive or negative sentiments).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    If multiple classification labels are available (`model.config.num_labels >= 2`), the pipeline will run a softmax\u001b[0m\n",
      "\u001b[0;34m    over the results. If there is a single label, the pipeline will run a sigmoid over the result.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See\u001b[0m\n",
      "\u001b[0;34m    the up-to-date list of available models on\u001b[0m\n",
      "\u001b[0;34m    [huggingface.co/models](https://huggingface.co/models?filter=text-classification).\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_all_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_model_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mTF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m \u001b[0mMODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_sanitize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Using \"\" as default argument because we're going to use `top_k=None` in user code to declare\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# \"No top_k\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpreprocess_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpostprocess_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"return_all_scores\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturn_all_scores\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mreturn_all_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_all_scores\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_legacy\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mreturn_all_scores\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"`return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\" `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mUserWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mreturn_all_scores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"function_to_apply\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Classify the text(s) given as inputs.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Args:\u001b[0m\n",
      "\u001b[0;34m            args (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`):\u001b[0m\n",
      "\u001b[0;34m                One or several texts to classify. In order to use text pairs for your classification, you can send a\u001b[0m\n",
      "\u001b[0;34m                dictionary containing `{\"text\", \"text_pair\"}` keys, or a list of those.\u001b[0m\n",
      "\u001b[0;34m            top_k (`int`, *optional*, defaults to `1`):\u001b[0m\n",
      "\u001b[0;34m                How many results to return.\u001b[0m\n",
      "\u001b[0;34m            function_to_apply (`str`, *optional*, defaults to `\"default\"`):\u001b[0m\n",
      "\u001b[0;34m                The function to apply to the model outputs in order to retrieve the scores. Accepts four different\u001b[0m\n",
      "\u001b[0;34m                values:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                If this argument is not specified, then it will apply the following functions according to the number\u001b[0m\n",
      "\u001b[0;34m                of labels:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                - If the model has a single label, will apply the sigmoid function on the output.\u001b[0m\n",
      "\u001b[0;34m                - If the model has several labels, will apply the softmax function on the output.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                Possible values are:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                - `\"sigmoid\"`: Applies the sigmoid function on the output.\u001b[0m\n",
      "\u001b[0;34m                - `\"softmax\"`: Applies the softmax function on the output.\u001b[0m\n",
      "\u001b[0;34m                - `\"none\"`: Does not apply any function on the output.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Return:\u001b[0m\n",
      "\u001b[0;34m            A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            - **label** (`str`) -- The label predicted.\u001b[0m\n",
      "\u001b[0;34m            - **score** (`float`) -- The corresponding probability.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            If `top_k` is used, one such dictionary is returned per label.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0m_legacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"top_k\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# This pipeline is odd, and return a list when single item is run\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenericTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# It used to be valid to use a list of list of list for text pairs, keeping this path for BC\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# This is likely an invalid usage of the pipeline attempting to pass text pairs.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"The pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m' dictionary `{\"text\": \"My text\", \"text_pair\": \"My pair\"}` in order to send a text pair.'\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_cache\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# `_legacy` is used to determine if we're running the naked pipeline and in backward\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# compatibility mode, or if running the pipeline with `pipeline(..., top_k=1)` we're running\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# the more natural result containing the list.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Default value before `set_parameters`\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGMOID\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOFTMAX\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"function_to_apply\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_to_apply\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGMOID\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOFTMAX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mfunction_to_apply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mClassificationFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unrecognized `function_to_apply` argument: {function_to_apply}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_legacy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdict_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdict_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mdict_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Classify the text(s) given as inputs.\n",
      "\n",
      "Args:\n",
      "    args (`str` or `List[str]` or `Dict[str]`, or `List[Dict[str]]`):\n",
      "        One or several texts to classify. In order to use text pairs for your classification, you can send a\n",
      "        dictionary containing `{\"text\", \"text_pair\"}` keys, or a list of those.\n",
      "    top_k (`int`, *optional*, defaults to `1`):\n",
      "        How many results to return.\n",
      "    function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      "        The function to apply to the model outputs in order to retrieve the scores. Accepts four different\n",
      "        values:\n",
      "\n",
      "        If this argument is not specified, then it will apply the following functions according to the number\n",
      "        of labels:\n",
      "\n",
      "        - If the model has a single label, will apply the sigmoid function on the output.\n",
      "        - If the model has several labels, will apply the softmax function on the output.\n",
      "\n",
      "        Possible values are:\n",
      "\n",
      "        - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      "        - `\"softmax\"`: Applies the softmax function on the output.\n",
      "        - `\"none\"`: Does not apply any function on the output.\n",
      "\n",
      "Return:\n",
      "    A list or a list of list of `dict`: Each result comes as list of dictionaries with the following keys:\n",
      "\n",
      "    - **label** (`str`) -- The label predicted.\n",
      "    - **score** (`float`) -- The corresponding probability.\n",
      "\n",
      "    If `top_k` is used, one such dictionary is returned per label."
     ]
    }
   ],
   "source": [
    "## again, with feeling\n",
    "classifier??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peeking inside the `pipeline`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the pipeline loaded the model: ``.  \n",
    "\n",
    "It then handled the three key pieces (Config, Preprocess, Model) underneath the hood. What exactly is `pipeline` doing?  \n",
    "\n",
    "Let's build or own pipeline from scratch, stepping one small level below the abstraction. To do this, we will create each of the key pieces manually.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertModel\n",
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model's name from up above and build each piece ourselves. HuggingFace uses the `from_pretrained` method to make this quick and easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model we are using\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the config\n",
    "config = DistilBertConfig.from_pretrained(model_name)\n",
    "\n",
    "# creating the preprocessor \n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# creating the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build a simple pipeline with these manual pieces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Sends `text` through the LLM's tokenizer.  \n",
    "    The tokenizers turns words and characters into special inputs for the LLM.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(text, return_tensors='pt')\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def forward(text):\n",
    "    \"\"\"\n",
    "    First we preprocess the `text` into tokens.\n",
    "    Then we send the `token_inputs` to the model.\n",
    "    \"\"\"\n",
    "    token_inputs = preprocess(text)\n",
    "    outputs = model(**token_inputs)\n",
    "    return outputs\n",
    "\n",
    "def process_outputs(outs):\n",
    "    \"\"\"\n",
    "    Here is where HuggingFace does the most for us via `pipeline`.  \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # grab the raw \"scores\" that from the model for Positive and Negative labels\n",
    "    logits = outs.logits\n",
    "\n",
    "    # find the strongest label score, aka the model's decision\n",
    "    pred_idx = logits.argmax(1).item()\n",
    "\n",
    "    # use the `config` object to find the class label\n",
    "    pred_label = config.id2label[pred_idx]  \n",
    "\n",
    "    # calculate the human-readable number for the score\n",
    "    pred_score = logits.softmax(-1)[:, pred_idx].item()\n",
    "\n",
    "    return {\n",
    "        'label': pred_label,\n",
    "        'score': pred_score, \n",
    "    }\n",
    "\n",
    "def simple_pipeline(text):\n",
    "    model_outs = forward(text)\n",
    "    preds = process_outputs(model_outs)\n",
    "    return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call this pipeline on the same example text from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We are very happy to show you the ðŸ¤— Transformers library.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'POSITIVE', 'score': 0.9997795224189758}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_pipeline(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Hugging Face Magic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Auto` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nghuyong/ernie-3.0-nano-zh\"\n",
    "tweet_classifier_model = \"vinai/bertweet-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(tweet_classifier_model)\n",
    "\n",
    "# For transformers v4.x+:\n",
    "tokenizer = AutoTokenizer.from_pretrained(tweet_classifier_model, use_fast=False)# It seems like the torch library is not imported. Let's import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT TWEET IS ALREADY NORMALIZED!\n",
    "line = \"SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = bertweet(input_ids)  # Models outputs are now tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0261,  0.2147,  0.1159,  ...,  0.0314,  0.0336, -0.1419],\n",
       "         [ 0.0986, -0.0205,  0.2265,  ...,  0.0828, -0.2993,  0.4767],\n",
       "         [-0.1011,  0.2133, -0.2283,  ...,  0.0797, -0.1762, -0.2632],\n",
       "         ...,\n",
       "         [-0.3448, -0.2996, -0.2430,  ..., -0.3354,  0.2429,  0.3029],\n",
       "         [-0.5279, -0.2429,  0.0758,  ..., -0.1733,  0.0389,  0.0513],\n",
       "         [-0.0425,  0.2355,  0.1219,  ...,  0.0018,  0.0732, -0.1305]]]), pooler_output=tensor([[ 2.1684e-01, -1.6747e-01, -3.9913e-02, -1.6048e-01,  1.2537e-01,\n",
       "         -7.5435e-02,  2.1886e-01, -1.3510e-01,  2.0169e-01, -2.1097e-01,\n",
       "         -9.4783e-02, -5.3385e-02, -2.2015e-01,  1.3985e-02,  1.5082e-01,\n",
       "         -6.2458e-02, -1.4717e-01, -6.7478e-02,  2.4149e-02,  1.7152e-01,\n",
       "         -1.1483e-01, -1.7264e-01,  2.9670e-01, -1.6612e-02, -2.3634e-02,\n",
       "          3.5211e-02, -1.6087e-01, -3.7129e-02,  4.9395e-02, -5.8116e-02,\n",
       "         -5.6990e-02, -1.5559e-01,  1.6120e-01,  5.6176e-02,  1.7266e-01,\n",
       "         -5.9471e-02,  2.1309e-01,  1.2466e-01, -1.2592e-01,  9.7800e-02,\n",
       "         -1.5980e-01, -4.7554e-02,  1.1069e-01,  2.2540e-02, -7.0860e-02,\n",
       "          3.4330e-02, -1.3061e-01,  3.6223e-02,  3.8687e-01, -4.9398e-02,\n",
       "         -6.2529e-03,  1.1077e-01, -6.5433e-02, -1.0200e-02, -5.4492e-03,\n",
       "          1.2476e-01, -5.6954e-03,  6.2194e-02, -2.2040e-01,  2.3035e-01,\n",
       "          4.0335e-02,  4.2737e-01, -2.5728e-01, -4.2542e-01, -1.2840e-02,\n",
       "         -1.3663e-01, -4.3941e-03,  1.1903e-01,  4.6278e-01,  2.3251e-01,\n",
       "          2.7170e-01,  1.0339e-01, -2.4488e-01, -1.2348e-02, -9.7118e-02,\n",
       "         -1.6682e-01,  6.5080e-02, -2.9772e-02,  1.3511e-01,  2.4833e-02,\n",
       "         -2.9946e-02, -3.5176e-01,  2.3219e-01, -9.5713e-02, -2.5918e-01,\n",
       "          2.1088e-01,  1.0469e-01,  6.2564e-02, -6.9195e-02,  2.1744e-01,\n",
       "         -1.8241e-01, -7.7701e-02, -1.3959e-01, -2.1408e-01, -1.0395e-01,\n",
       "         -2.8095e-02,  2.9942e-04,  2.3257e-01, -1.2731e-01, -1.7985e-01,\n",
       "         -1.8892e-01,  8.1731e-02, -3.7738e-03,  6.8216e-03,  4.1659e-03,\n",
       "         -8.9084e-02, -3.5341e-02,  2.2507e-01,  2.8873e-02, -7.7968e-02,\n",
       "         -2.0763e-02, -1.6699e-01, -2.2962e-01, -1.6147e-01,  1.9951e-01,\n",
       "         -2.0290e-02, -3.0334e-01,  2.8734e-01,  2.3261e-01,  7.0353e-02,\n",
       "         -3.4001e-02,  2.0040e-01,  1.5577e-02,  3.0432e-01,  5.7032e-02,\n",
       "         -3.4262e-02,  6.6559e-02, -6.2958e-02,  1.2308e-01, -2.3498e-01,\n",
       "          5.8435e-02, -1.7375e-02,  1.5279e-01, -1.6965e-01,  1.1444e-01,\n",
       "         -1.9653e-02,  1.4225e-01,  8.3226e-02,  1.7470e-01,  1.6492e-01,\n",
       "          2.9033e-03, -2.9698e-01, -1.7966e-02,  3.3161e-01,  2.6770e-03,\n",
       "          3.0237e-01, -1.2641e-01,  1.4140e-01, -9.1238e-02, -4.3958e-02,\n",
       "         -2.0406e-03, -1.0386e-01, -1.9405e-01,  1.6783e-01, -4.5707e-02,\n",
       "         -8.6509e-02,  4.4101e-04,  6.7585e-02,  5.1672e-02, -9.1920e-02,\n",
       "          2.1051e-01, -2.8929e-01,  1.1323e-02, -1.5257e-01,  7.3889e-02,\n",
       "         -7.6998e-02, -1.5864e-01, -5.7710e-02, -1.4706e-01, -2.0447e-02,\n",
       "         -1.4777e-01, -1.7777e-01, -6.2066e-01, -2.8002e-01,  2.0328e-01,\n",
       "          1.6720e-01, -1.3829e-01,  9.3785e-02,  1.9930e-01,  6.8548e-02,\n",
       "         -8.3035e-03,  1.0518e-01, -2.2478e-01,  3.6735e-02, -1.4841e-01,\n",
       "          2.2288e-01, -9.9642e-02, -9.6726e-02,  6.0620e-02,  3.3674e-02,\n",
       "          7.1847e-02, -1.7697e-02,  2.5305e-01,  1.8194e-01, -3.8007e-01,\n",
       "         -1.7328e-01,  1.5387e-01, -1.9752e-01,  5.8195e-02,  1.4141e-01,\n",
       "          1.7599e-01,  9.9589e-02, -1.7539e-01,  3.9723e-02,  5.7362e-02,\n",
       "         -6.0153e-03, -7.4778e-02, -3.9814e-02,  4.2532e-03, -6.1438e-03,\n",
       "         -2.7549e-01, -1.7463e-01, -1.0434e-01, -2.8234e-01, -1.1606e-01,\n",
       "          2.2520e-01, -2.6030e-01, -5.6626e-02, -4.5074e-01,  2.0554e-01,\n",
       "          2.4415e-01, -2.4389e-01,  1.4430e-01,  2.7835e-02, -8.4409e-02,\n",
       "         -5.5983e-04,  4.9588e-01,  7.3265e-02, -1.3965e-01, -3.0254e-01,\n",
       "         -3.4468e-02, -8.2590e-02,  1.2788e-01, -1.6157e-02,  1.7172e-01,\n",
       "          6.1288e-01,  1.5453e-01,  9.4416e-02, -1.1867e-01, -2.8974e-02,\n",
       "         -2.2069e-01, -1.8120e-01,  9.5515e-02, -1.8660e-01,  3.1271e-02,\n",
       "          3.6478e-02,  4.3989e-01,  5.9990e-02, -2.1676e-01,  9.2674e-02,\n",
       "         -1.1439e-01, -2.4943e-01, -2.9657e-01,  1.3715e-01, -5.5204e-02,\n",
       "          8.1869e-02, -2.5680e-03,  4.9731e-03,  2.9959e-01,  1.8850e-01,\n",
       "          9.5238e-02,  1.7245e-01, -2.8302e-01, -1.0291e-01, -1.3128e-01,\n",
       "         -7.4131e-02,  6.4492e-02, -2.6746e-01, -7.7197e-02, -1.6304e-01,\n",
       "         -2.2387e-01, -1.2678e-01,  6.6664e-02, -2.5839e-02, -8.8316e-02,\n",
       "         -2.3827e-01, -6.9970e-03,  1.8078e-01, -6.2181e-02, -3.8105e-01,\n",
       "          1.2478e-02,  2.3644e-01, -2.2813e-01, -6.5327e-02, -4.2093e-02,\n",
       "         -4.9957e-02, -1.6963e-02,  2.8927e-01,  1.2748e-01, -1.6388e-02,\n",
       "         -1.4192e-01, -1.0516e-01, -7.7114e-03,  3.0210e-01,  7.8765e-02,\n",
       "         -2.9639e-01,  1.6735e-01,  1.0467e-02,  2.1078e-01,  7.6494e-02,\n",
       "         -1.1333e-01, -1.7217e-01,  1.1373e-01, -2.2313e-01, -1.2148e-01,\n",
       "         -1.9967e-01, -1.0848e-01, -1.8277e-03,  1.9473e-01,  1.9243e-01,\n",
       "          1.7690e-01, -2.0756e-02,  2.1948e-01,  2.6942e-01, -6.2986e-03,\n",
       "          1.4327e-01,  1.1615e-01,  2.0209e-01,  3.8537e-02, -4.6930e-02,\n",
       "         -7.8353e-02, -1.7991e-01, -1.6937e-01, -4.8626e-02,  2.6269e-02,\n",
       "          1.7385e-02, -1.7453e-01,  2.6845e-02, -5.6886e-02,  2.7073e-01,\n",
       "          2.8761e-01, -1.0229e-01,  1.4652e-01, -8.4829e-02, -1.5249e-01,\n",
       "         -1.6665e-01,  1.6805e-01,  7.7209e-02,  5.8676e-02,  4.0058e-02,\n",
       "         -1.5571e-01, -1.2718e-01,  3.1281e-01, -9.1534e-03,  1.3576e-01,\n",
       "          3.9473e-01,  2.4109e-02, -3.8084e-02, -1.4643e-01,  2.8824e-02,\n",
       "         -3.2364e-02, -1.4315e-01,  3.7451e-02, -4.4504e-02,  1.5163e-01,\n",
       "         -2.8780e-01,  1.6395e-01,  1.3348e-01, -2.2630e-01,  7.0919e-02,\n",
       "         -2.1954e-01,  2.5653e-02, -8.2631e-02, -1.2159e-01, -1.5556e-01,\n",
       "          7.7520e-02, -1.2948e-01, -2.3807e-01,  2.7228e-01, -4.2869e-02,\n",
       "         -7.8903e-03,  8.9008e-02,  3.2700e-01, -1.7110e-01, -1.3331e-01,\n",
       "          2.4333e-02,  2.5774e-02, -3.7812e-01, -1.7119e-01, -7.4228e-02,\n",
       "          1.4139e-01, -1.0524e-01, -2.4698e-01,  1.0323e-01, -1.0664e-01,\n",
       "          9.5311e-02, -3.6480e-01,  1.9375e-01,  2.4034e-01, -1.3027e-01,\n",
       "          2.6347e-01, -2.1574e-02,  7.0733e-03, -4.5205e-01,  1.0983e-01,\n",
       "          7.4580e-02,  1.9370e-01,  5.6969e-02,  3.0360e-01, -1.0367e-01,\n",
       "         -6.9653e-02, -2.0863e-01,  1.1197e-01, -8.3405e-02, -4.3823e-02,\n",
       "         -1.0581e-01, -2.6405e-03,  1.2920e-01,  7.2932e-02, -8.2181e-03,\n",
       "         -1.5625e-01,  2.5517e-01, -3.2143e-01, -1.0550e-01, -8.6524e-02,\n",
       "          1.3654e-01,  2.1519e-01,  3.3233e-02,  1.1481e-01, -2.7192e-01,\n",
       "          1.5108e-01, -1.2987e-01,  5.7868e-02,  4.9990e-02, -5.4458e-02,\n",
       "          2.7341e-02, -1.0211e-01, -7.1317e-02, -1.0423e-01, -4.7862e-02,\n",
       "         -1.6483e-02, -1.6297e-01, -1.1742e-01, -1.3970e-02, -9.0472e-02,\n",
       "          3.6632e-02,  5.2472e-02,  1.0371e-01,  5.5842e-02, -1.8505e-01,\n",
       "         -1.7271e-01, -3.4367e-01,  2.3774e-01, -2.4568e-01, -6.8554e-02,\n",
       "         -1.1043e-01, -2.1823e-01, -3.3877e-01, -1.8825e-01, -1.3074e-01,\n",
       "          2.0080e-01, -4.3499e-02, -1.1833e-03, -1.0494e-02,  9.8839e-02,\n",
       "         -9.8610e-02,  1.0916e-01, -1.2520e-01,  2.2091e-01,  1.8034e-01,\n",
       "          1.3691e-01,  5.8596e-02,  9.1620e-02,  2.5686e-01, -1.6616e-01,\n",
       "         -2.1491e-01,  1.5801e-01, -6.9085e-02, -1.1327e-01,  2.2038e-01,\n",
       "          4.0991e-02, -1.2667e-01, -2.1626e-01,  9.1708e-02, -2.1187e-01,\n",
       "          3.8479e-02,  1.3006e-02,  2.8096e-01,  5.8074e-02,  6.4363e-02,\n",
       "         -1.9326e-01,  5.0338e-02, -1.6850e-01,  2.5039e-01, -1.2964e-01,\n",
       "         -4.5641e-02, -1.9423e-01, -2.0506e-01,  2.7561e-01, -2.1644e-01,\n",
       "          1.3510e-01, -1.5607e-01, -1.9618e-01,  3.6909e-01,  1.6661e-02,\n",
       "         -2.7980e-01,  1.6374e-01,  9.8737e-02, -4.2289e-02, -5.9343e-03,\n",
       "         -2.3921e-01, -9.5844e-02,  7.9161e-02,  4.8087e-02, -1.5758e-01,\n",
       "         -3.2436e-01, -2.1874e-01,  9.2943e-02, -1.7660e-01,  3.2471e-01,\n",
       "          4.1639e-02, -8.4214e-02, -1.1156e-01,  1.7573e-01, -1.4107e-01,\n",
       "          9.6379e-02, -8.0719e-02, -2.1352e-01,  1.9738e-01, -7.8069e-04,\n",
       "         -1.4115e-01, -1.4976e-01, -1.0283e-01,  2.7926e-01,  2.2314e-01,\n",
       "          2.6503e-01, -2.9287e-01,  5.3031e-03, -1.6676e-01, -1.7062e-01,\n",
       "          1.6481e-01, -6.6617e-02,  1.3794e-01,  1.2838e-01,  1.7721e-01,\n",
       "         -3.6212e-01,  1.9786e-03, -4.0427e-02, -1.4486e-01, -9.7480e-02,\n",
       "         -1.8925e-01,  4.2498e-01, -5.7728e-02, -4.3533e-02,  1.7206e-02,\n",
       "          1.3141e-01, -2.7031e-02, -1.0911e-01,  3.1601e-01,  5.3195e-02,\n",
       "          2.1432e-01, -1.2668e-01,  8.3884e-02, -3.1696e-01, -1.4914e-01,\n",
       "         -3.0770e-01,  5.2150e-01, -1.0679e-01,  1.0872e-01, -3.0105e-02,\n",
       "         -1.8292e-01,  2.4955e-01,  4.8210e-02,  6.5361e-02, -8.1284e-02,\n",
       "         -7.3967e-02,  2.1774e-01, -2.5905e-01, -9.9748e-02,  2.2861e-01,\n",
       "          2.0665e-01, -6.5433e-02, -1.4660e-01, -2.7731e-02, -3.7495e-01,\n",
       "         -3.3564e-01,  2.2604e-01,  1.2786e-01, -1.1509e-01, -3.2010e-01,\n",
       "          5.0551e-02, -5.2174e-02, -6.8957e-02, -3.2839e-01, -1.4177e-01,\n",
       "          1.2305e-01,  5.3620e-02, -2.2995e-01, -1.0491e-01, -7.8523e-02,\n",
       "         -5.8274e-02, -3.0538e-01,  2.1489e-01,  1.6900e-01,  1.0598e-01,\n",
       "          1.4898e-03, -8.4836e-02, -2.2781e-01,  6.4068e-02,  6.7883e-02,\n",
       "          5.4033e-01,  7.4442e-02,  2.1099e-01, -2.4285e-01, -9.5264e-02,\n",
       "         -7.1959e-02,  7.2113e-02,  1.9963e-01,  7.2252e-02, -1.8008e-01,\n",
       "          1.1053e-02,  7.1147e-03,  1.3077e-01, -1.1621e-01,  1.3477e-01,\n",
       "          1.8277e-01, -1.2061e-01,  2.5727e-01, -4.8499e-02, -7.5046e-02,\n",
       "         -1.1551e-01,  5.9584e-02,  1.1659e-01, -2.9556e-01,  3.4345e-02,\n",
       "          2.3334e-01, -3.8725e-02, -4.8951e-02,  1.2001e-01, -1.2271e-01,\n",
       "          1.2176e-01,  1.7248e-01,  1.7510e-01, -1.0586e-01,  1.4408e-01,\n",
       "          4.6973e-02, -1.9982e-01, -3.1675e-01,  1.8192e-01, -4.9504e-02,\n",
       "         -1.1049e-01,  5.1719e-02, -3.8881e-01, -3.7701e-01, -2.3664e-01,\n",
       "          6.1054e-02, -1.6371e-01, -1.8980e-02, -1.0350e-02, -1.8236e-02,\n",
       "         -3.1771e-01,  1.7057e-02, -6.7176e-02,  3.0066e-02,  1.2955e-01,\n",
       "          2.5587e-02, -1.3062e-02,  1.5314e-01,  1.2715e-01,  2.5056e-01,\n",
       "         -2.6158e-01, -7.3815e-02, -2.8139e-01, -6.8489e-02, -2.4890e-01,\n",
       "         -1.0459e-01,  2.1866e-01, -1.3652e-02,  2.2082e-01,  1.4452e-01,\n",
       "          1.2156e-01, -4.2408e-03,  2.5653e-01, -1.4201e-02, -1.3602e-01,\n",
       "          2.4607e-02, -2.4028e-01,  9.0389e-02, -2.0443e-01, -2.6975e-01,\n",
       "          2.9034e-01, -8.2160e-02, -4.7939e-02,  2.1328e-01,  4.1331e-02,\n",
       "         -1.3397e-01, -5.8570e-02, -1.4730e-01,  2.0228e-01,  2.1142e-01,\n",
       "          1.9471e-01, -9.2305e-02,  2.7115e-02,  2.2871e-02, -2.3399e-01,\n",
       "         -6.6573e-02, -3.1556e-02, -6.7305e-02, -9.1438e-02, -2.6135e-01,\n",
       "         -7.6051e-02, -1.3300e-01,  2.3206e-01,  2.1293e-01, -4.6348e-02,\n",
       "         -1.7028e-01, -3.1770e-02,  3.3346e-03, -7.7843e-03,  2.5576e-01,\n",
       "          4.3425e-02,  1.1490e-02, -9.8304e-02, -2.2572e-01, -5.6736e-02,\n",
       "          2.4170e-01, -8.3043e-02, -3.2750e-01, -1.8894e-01, -7.3210e-02,\n",
       "          2.4887e-02, -5.6985e-02,  3.2579e-01, -7.0417e-02,  5.7850e-02,\n",
       "         -5.1130e-02, -6.9433e-02, -1.9004e-01,  1.9591e-02, -1.7293e-01,\n",
       "         -1.5911e-01,  9.1796e-02,  3.4482e-01, -8.8054e-03,  9.0982e-02,\n",
       "         -2.2528e-02, -1.4741e-01,  5.4736e-02, -2.4447e-01, -1.3338e-01,\n",
       "         -1.2902e-01, -1.2377e-01, -2.4834e-01, -2.0397e-02, -4.5321e-03,\n",
       "          1.2454e-01, -4.7694e-02,  1.7699e-01, -3.2227e-01,  2.7142e-01,\n",
       "         -1.2625e-01, -2.3601e-01,  9.6091e-02,  3.3450e-01, -1.4860e-01,\n",
       "         -1.3938e-01,  1.1786e-01,  2.0373e-02, -1.1923e-01, -2.8852e-01,\n",
       "         -2.2158e-01,  1.6883e-01,  1.2095e-01,  3.1874e-01, -3.6485e-01,\n",
       "          1.2620e-02, -6.6986e-02, -1.9560e-01]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
