{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setups\n",
    "\n",
    "> Run a Large Language Model using the [HuggingFace `Transformers`](https://huggingface.co/docs/transformers/index) API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lesson_1.first_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are good defaults for development.  \n",
    "\n",
    "The `autoreload` lines help load libraries on the fly, while they are changing. This works well with the editable install we created via `pip install -e .`  \n",
    "This means we can edit the source code directly and have the change reflected live in the notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a list of product review from our users. Now we want to find out whether those reviews were good or bad. It will take a lot of effort to manually go through and check each one. But, using an LLM, we can automatically get a label for a given product review. \n",
    "\n",
    "How would this be useful? We could use it to find the more negative reviews to see where our product needs improving. Or, we can look at the more positive ones to see what we're doing right.  \n",
    "\n",
    "The broader task in NLP of figuring out a statement's tone is called `Sentiment Analysis`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A HuggingFace model is based on 3 key pieces: \n",
    "1. Config file.  \n",
    "2. Preprocessor file.   \n",
    "3. Model file.   \n",
    "\n",
    "The HuggingFace API gives us a way of automatically using these pieces directly: the `pipeline`.  \n",
    "\n",
    "Let's get right it and create a Sentiment Analysis `pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "#| export \n",
    "\n",
    "# load in the pipeline object from huggingface\n",
    "from transformers import pipeline\n",
    "\n",
    "# create the sentiment analysis pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the output message above that HuggingFace automatically picked a decent, default model for us since we didn't specify one. Specifically, it chose a [distilbert model](distilbert-base-uncased-finetuned-sst-2-english).  \n",
    "\n",
    "We will learn more about what exactly `distilbert` is and how it works later on. For now, think of it as a useful NLP genie who can tell us how it feels about a given sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "# example from the HuggingFace tutorial\n",
    "classifier(\"We are very happy to show you the ðŸ¤— Transformers library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n",
      "label: POSITIVE, with score: 0.999\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "# passing in several sentences at once, inside a python list\n",
    "results = classifier([\n",
    "    \"We are very happy to show you the ðŸ¤— Transformers library.\",\n",
    "    \"We hope you don't hate it.\",\n",
    "    \"I love Fractal! I'm so glad it's not a cult!\", \n",
    "])\n",
    "\n",
    "# print the output of each results\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the `classifier`, notebook style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the `classifier`, exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x2a97edd90>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## showing the lookup's auto-complete\n",
    "# classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_batch_size',\n",
       " '_ensure_tensor_on_device',\n",
       " '_forward',\n",
       " '_forward_params',\n",
       " '_num_workers',\n",
       " '_postprocess_params',\n",
       " '_preprocess_params',\n",
       " '_sanitize_parameters',\n",
       " 'binary_output',\n",
       " 'call_count',\n",
       " 'check_model_type',\n",
       " 'default_input_names',\n",
       " 'device',\n",
       " 'device_placement',\n",
       " 'ensure_tensor_on_device',\n",
       " 'feature_extractor',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'function_to_apply',\n",
       " 'get_inference_context',\n",
       " 'get_iterator',\n",
       " 'image_processor',\n",
       " 'iterate',\n",
       " 'model',\n",
       " 'modelcard',\n",
       " 'postprocess',\n",
       " 'predict',\n",
       " 'preprocess',\n",
       " 'return_all_scores',\n",
       " 'run_multi',\n",
       " 'run_single',\n",
       " 'save_pretrained',\n",
       " 'task',\n",
       " 'tokenizer',\n",
       " 'torch_dtype',\n",
       " 'transform']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## viewing all of a class' methods and properties\n",
    "# dir(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebooks have powerful ways of inspecting and analyzing the code, as we're running it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## refresher\n",
    "# classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the power of asking questions\n",
    "# classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## again, with feeling\n",
    "# classifier??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peeking inside the `pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the pipeline loaded the model: ``.  \n",
    "\n",
    "It then handled the three key pieces (Config, Preprocess, Model) underneath the hood. What exactly is `pipeline` doing?  \n",
    "\n",
    "Let's build or own pipeline from scratch, stepping one small level below the abstraction. To do this, we will create each of the key pieces manually.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertModel\n",
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model's name from up above and build each piece ourselves. HuggingFace uses the `from_pretrained` method to make this quick and easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model we are using\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the config\n",
    "config = DistilBertConfig.from_pretrained(model_name)\n",
    "\n",
    "# creating the preprocessor \n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# creating the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build a simple pipeline with these manual pieces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Sends `text` through the LLM's tokenizer.  \n",
    "    The tokenizers turns words and characters into special inputs for the LLM.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(text, return_tensors='pt')\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def forward(text):\n",
    "    \"\"\"\n",
    "    First we preprocess the `text` into tokens.\n",
    "    Then we send the `token_inputs` to the model.\n",
    "    \"\"\"\n",
    "    token_inputs = preprocess(text)\n",
    "    outputs = model(**token_inputs)\n",
    "    return outputs\n",
    "\n",
    "def process_outputs(outs):\n",
    "    \"\"\"\n",
    "    Here is where HuggingFace does the most for us via `pipeline`.  \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # grab the raw \"scores\" that from the model for Positive and Negative labels\n",
    "    logits = outs.logits\n",
    "\n",
    "    # find the strongest label score, aka the model's decision\n",
    "    pred_idx = logits.argmax(1).item()\n",
    "\n",
    "    # use the `config` object to find the class label\n",
    "    pred_label = config.id2label[pred_idx]  \n",
    "\n",
    "    # calculate the human-readable number for the score\n",
    "    pred_score = logits.softmax(-1)[:, pred_idx].item()\n",
    "\n",
    "    return {\n",
    "        'label': pred_label,\n",
    "        'score': pred_score, \n",
    "    }\n",
    "\n",
    "def simple_pipeline(text):\n",
    "    model_outs = forward(text)\n",
    "    preds = process_outputs(model_outs)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call this pipeline on the same example text from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We are very happy to show you the ðŸ¤— Transformers library.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'POSITIVE', 'score': 0.9997795224189758}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_pipeline(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
